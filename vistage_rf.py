# -*- coding: utf-8 -*-
"""vistage_RF.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MX2060lSfewEkn7Y1jozkmngy-rcx-eq
"""

!pip install -U -q PyDrive

import os
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# 1. Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# choose a local (colab) directory to store the data.
local_download_path = os.path.expanduser('~/data')
try:
  os.makedirs(local_download_path)
except: pass

# 2. Auto-iterate using the query syntax

#for model importing

#    https://developers.google.com/drive/v2/web/search-parameters

#    Put the file ID from the get shareable link tab

file_list = drive.ListFile(
    {'q': "'1q18Fx9Wf87vCzFEmwYKxwie5GHorYbDu' in parents"}).GetList()

for f in file_list:
  # 3. Create & download by id.
  print('title: %s, id: %s' % (f['title'], f['id']))
  fname = os.path.join(local_download_path, f['title'])
  print('downloading to {}'.format(fname))
  f_ = drive.CreateFile({'id': f['id']})
  f_.GetContentFile(fname)

from google.colab import files
uploaded = files.upload()

from google.colab import files
uploaded = files.upload()

import pandas as pd
import random

train_pd=pd.read_excel(r"Train_data (1).xlsx")
#train_pd.drop(columns=[train_pd.columns[21]],inplace=True)
print(train_pd.columns)
#train_pd=pd.read_excel(r"/root/data/data_shiv_3.xlsx")
train_full=train_pd.to_numpy()
test_pd=pd.read_excel(r"Test_data (1).xlsx")
#test_pd.drop(columns=[test_pd.columns[21]],inplace=True)
test_full=test_pd.to_numpy()

random.shuffle(train_full)
print(train_full.shape)
print(test_full.shape)

import numpy as np

split=np.int(np.round(0.85*train_full.shape[0]))
train_x,train_y=train_full[:split,:-1],train_full[:split,-1]
train_y=train_y.reshape(train_y.shape[0],1)

val_x,val_y=train_full[split:,:-1],train_full[split:,-1]
val_y=val_y.reshape(val_y.shape[0],1)

train_x=train_x[:,2:]
val_x=val_x[:,2:]
test_x=test_full[:,1:]


# ##Normalization step

# train_x=normalize(train_x[:,1:])*10
# val_x=normalize(val_x[:,1:])*10
# test_x=test_full[:,1:]


# scaler=MinMaxScaler()
# train_x=scaler.transform(train_x)
# val_x=scaler.transform(val_x)

print(train_x.shape , val_x.shape)

print(train_y .shape, val_y.shape)

# import numpy as np
# train_buf= pd.read_excel(r"/root/data/data_shiv_3.xlsx",dtype=object)
# #train_buf2=train_buf.iloc[:,14:15].astype(int)
# print(train_buf.columns[14:15])
# #train_buf.iloc[:,14]



######Random Forest Classfiers

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score,accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.ensemble import bagging
clf=RandomForestClassifier(max_depth=100,max_features=5)
fit=clf.fit(np.array(train_x,dtype=int),np.array(train_y,dtype=int))

# print(clf.feature_importances_)
pred=clf.predict(val_x)
pred_prob=clf.predict_proba(val_x)
pred_prob_train=clf.predict_proba(train_x)


confusion=confusion_matrix(np.array(val_y,dtype=int),pred)
roc=roc_auc_score(np.array(val_y,dtype=int),pred_prob[:,1])
mean_accuracy=clf.score(val_x,np.array(val_y,dtype=int))
mean_acc_train=clf.score(train_x,np.array(train_y,dtype=int))
print("MEAN Acuuracy=",mean_acc_train,mean_accuracy)
print("ROC",roc)
print(confusion)

####### GBM Classifier


from sklearn.ensemble import GradientBoostingClassifier
clf=GradientBoostingClassifier(learning_rate=0.1,random_state=10,subsample=0.2)
fit=clf.fit(train_x,np.array(train_y,dtype=int),)

pred=clf.predict(val_x)
pred_prob=clf.predict_proba(val_x)
pred_prob_train=clf.predict_proba(train_x)

confusion=confusion_matrix(np.array(val_y,dtype=int),pred)
roc=roc_auc_score(np.array(val_y,dtype=int),pred_prob[:,1])
mean_accuracy=clf.score(val_x,np.array(val_y,dtype=int))
mean_acc_train=clf.score(train_x,np.array(train_y,dtype=int))
print("MEAN Acuuracy=",mean_acc_train,mean_accuracy)
print("ROC_val",roc)
#print(confusion)
importance_array=clf.feature_importances_
feature_matrix=pd.DataFrame(columns=["label","value"])
feature_matrix["label"]=train_pd.columns[2:29]
feature_matrix["value"]= importance_array

#feature_matrix.sort_values("value",ascending=False)

####Test on Gradient Boosting Method
print(test_x.shape)
test_prediction=clf.predict(test_x)
test_prediction_prob=clf.predict_proba(test_x)

##Print test data
test=pd.DataFrame(test_full)
test["predicted value"] = test_prediction
print(np.sum(test_prediction))
test["health score"]= test_prediction_prob[:,0]
#print test data
writer = pd.ExcelWriter('test_predictions_CE.xlsx')
test.to_excel(writer)
writer.save()
from google.colab import files
files.download('test_predictions_CE.xlsx')

#print feature matrix
writer = pd.ExcelWriter('feature_matrix_CE.xlsx')
feature_matrix.to_excel(writer)
writer.save()
from google.colab import files
files.download('feature_matrix_CE.xlsx')



dataframe2=pd.DataFrame(train_x)
dataframe2.shape
dataframe2["probability_to_churn"]=pred_prob_train[:,1]
dataframe2["Status"]=np.array(train_y,dtype=int)

writer = pd.ExcelWriter('train_results_14_oct.xlsx')
dataframe2.to_excel(writer)
writer.save()
from google.colab import files
files.download('train_results_14_oct.xlsx')

writer = pd.ExcelWriter('train_data_with_results_13_oct.xlsx')
dataframe2.to_excel(writer)
writer.save()
from google.colab import files
files.download('train_data_with_results_13_oct.xlsx')

pd.Series(pred_prob_train)